# 1. 基础和应用篇

## 1.1 Redis基础数据结构

Redis有5种基础数据结构，分别为：string（字符串）、list（列表）、hash（字典）、set（集合）和zset（有序集合）

### 1.1.1 String

- Redis字符串是动态字符串，可修改。

- 实现方式类似Java的ArrayList，通过预分配冗余空间来减少频繁的内存分配。
- 字符串长度小于1MB时，扩容是加倍现有空间。长度超过1MB时，每次扩容增加1MB的空间，字符串最大长度512MB。

### 1.1.2 List

- Redis列表相当于Java里面的LinkedList（实际底层是一种“quicklist”的快速列表），插入和删除时间复杂度为O(1)，定位查找时间复杂度O(n)。
- 元素之间使用双向指针连接，支持向前向后遍历。
- 常用来做异步队列，将任务序列化成字符串放入Redis的列表，别的线程从这个列表中轮询数据进行处理。
- ziplist是使用一块连续内存来实现链表，减少前后指针也就减少了内存使用。quicklist是将若干个ziplist双向连接起来。

### 1.1.3 Hash

- 相当于Java里的HashMap，数组+链表二维结构。
- key只能是字符串。
- 采用渐进式rehash策略，在rehash时，保留新旧两个hash，查询时会同时查两个hash，后续会定时将数据从旧的hash里移动到新的hash，迁移完成就会用新的hash代替旧的，然后回收其空间。

### 1.1.4 Set

- Redis集合相当于Java语言里面的HashSet。
- 内部实现是一个value为NULL的字典，所有的键值对是无序的，唯一的。

### 1.1.5 zset

- zset，有序列表，类似于Java的SortedSet和HashMap的结合体。
- 它是一个set，可以保证内部元素(value)的唯一性。
- 可以对set里的value，根据其score进行排序。
- 内部实现是由hash字典加上一种叫做“跳表”的数据结构。

### 1.1.6 容器型数据结构通用规则

1. create if not exists：如果容器不存在就先创建一个，再进行操作。
2. drop if no elements：如果容器里没有元素，就立即删除容器，释放内存。

### 1.1.7 过期时间

Redis的所有对象都可以设置过期时间，时间到了，就会自动删除该对象。过期是以对象为单位，比如一个hash结构，过期指的是整个hash对象，而不是里面的某个key。

## 1.2 分布式锁

分布式锁是用来解决分布式应用之间的同步问题。使用Redis来实现分布式锁。

```
> setnx lock true
// 如果之前不存在lock变量，则返回1，相当于抢到了锁，否则返回0。
> del lock
// 用完之后删除lock，相当于释放锁。
```

### 1.2.1 死锁问题

如果加锁成功后，在释放锁之前出现异常，导致del指令不能被调用，这样锁永远都得不到释放。解决方法就是，在拿到锁之后，再给锁加一个过期时间，这样即使中途发生异常也可以保证锁到期后被释放。

```
> setnx lock true
> expire lock 5
···do something···
> del lock
```

但是仍然存在问题，就是拿到锁后，在设定锁过期时间之前发生异常，还是会造成死锁。

解决方法是把加锁和设置过期时间变成原子操作。

```
> set lock true ex5 nx
···do something···
> del lock 
```

### 1.2.2 超时问题

Redis分布式锁并不能解决超时问题，如果在加锁和释放锁之间的逻辑执行得太久，以至于超出了锁的超时限制，就会出现问题，导致临界区代码不能得到严格串行执行。为了避免这个问题，Redis分布式锁不要用于较长时间的任务。

### 1.2.3 可重入性

Redis要实现可重入性，可以对set方法进行包装，使用线程的Threadlocal变量存储当前持有锁的计数。

## 1.3 延时队列

Redis可以用来实现只有一个消费者组的消息队列，但是可靠性没有保障。使用list数据结构来作为一部消息队列使用，用rpush和lpush操作入队列，用lpop和rpop操作出队列。支持多个生产者和多个消费者并发进出消息，每个消费者拿到的消息都是不同的列表元素。

### 1.3.1 队列空了怎么办

队列空了，客户端会陷入pop的死循环，导致cpu消耗升高，通常使用sleep来解决这个问题，让线程睡一会儿再请求数据。

### 1.3.2 阻塞读

睡眠虽然可以解决空轮询问题，但是会导致消息延迟增大。因此更好的解决方案是阻塞读。使用命令`blpop/brpop`，阻塞读在队列没有数据的时候，会立即进入休眠状态，一旦数据到来，则立刻醒过来，消息的延迟几乎为0。

### 1.3.3 空闲连接自动断开

如果阻塞读长时间阻塞，会导致长时间占用资源，因此在阻塞了一段时间后，blpop/brpop会抛出异常。

### 1.3.4 延时队列的实现

延时队列可以通过redis的zset有序列表来实现。将消息序列化成一个字符串作为zset的value，将消息的到期处理时间作为score，然后用多个线程轮询zset获取到期的任务进行处理。使用redis.zrangebyscore("delay-queue, 0, time.time(), start=0, num=1")来获取一条满足到期时间的任务。然后使用redis.zrem来抢这条任务，因为多个线程都可以使用pop命令同时读到该消息，只有这一步成功了才算抢到了任务，可以执行该任务。

### 1.3.5 进一步优化

在上面的算法中，同一个任务可能会被多个进程取到之后再使用zrem进行争抢，那些没抢到的进程都白取了一次任务，这是浪费。因此可以考虑用lua scripting来优化，将zrangbyscore和zrem一同挪到服务端进行原子化操作，这样就不会出现这种浪费了。

## 1.4 位图

Redis中实现位图的方式是`setbit/getbit`命令。

```
> setbit s 1 1
> getbit s 1
> 1
> set w h
> getbit w 1
> 1
```

### 1.4.1 统计和查找

`bitcount key [start] [end]` 统计指定区间上1的个数

`bitpos key bit [start] [end]`返回变量key中第start个到end个字符中，第一个`bit`的位置。

### 1.4.2 bitfield

bitfield指令用于一次处理多个位的操作，最多同时处理64位。

 ```
> bitfield w get u4 0  // 从第一位开始取4个位，结果是无符号数（u）
> bitfield w get u4 0  // 从第一位开始取4个位，结果是有符号数（i）
> bitfield w set u8 8 97 // 从第9个位开始，将接下来的8个位用无符号数97替换
 ```

incrby用来对指定范围的位进行自增操作。对于溢出，Redis默认是折返，如果溢出，会将溢出的符号位丢掉。bitfield提供了溢出策略子命令overflow，可以选择失败(fail)以及饱和截断(sat)，饱和截断就是保持最大值 。

##1.5 HyperLogLog

HyperLogLog是一种set，提供不精确的去重技术方案，标准误差是0.81%，每个HyperLogLog占用12K的内存。

`pfadd`增加计数，`pfcount`获取计数，`pfmerge`将多个pf计数值累加到一起。

原理：

给定一系列的随机正数，记录下低位连续零位的最大长度K，通过K值可以估算随机数的数量N。HyperLogLog使用了16384个桶来计数，每个桶的maxbits使用6个bit进行计数，最大可以表示maxbits=64，因此占用12K内存。

1. 每到来一个随机数，先确定它放到某一个桶中，这里同时也实现了去重功能。
2. 求出它的低位连续零位，和当前这个桶的最大长度maxbits作比较。
3. 使用调和平均（倒数的平均）来计算平均数，这样能有效防止个别离群值对平均结果产生较大影响，最后根据桶的数量对估值进行放大。

## 1.6 布隆过滤器

### 1.6.1 是什么

布隆过滤器，从名字上看，是一种过滤器，就像茶叶过滤器一样，过滤掉的一定是茶叶，不可能是水，而留下来的可能有细小的茶叶，同样的布隆过滤器也有微小的误判率。

从数据结构上来说，它是一种高级set，提供set的add, contains函数，不同的是，首先，元素加进去了，不能再取出来了，add方法只是让布隆过滤器见一面这个元素，而不是真正存储起来，而contains函数告诉调用者，有没有见过这个元素，当他说没有，那就一定没有见过，当他说有，那就可能见过，可能没见过。第二，相比于set，布隆过滤器能节省90%空间。

### 1.6.2 能做什么

布隆过滤器提供不完全正确的去重检查功能，适用的场景有以下几种：

- 判断用户有没有读过谋篇新闻，将用户浏览过的所有新闻都加到布隆过滤器里面，当有一篇新的新闻来的时候，去过滤器里面查一查，如果说不存在，则用户一定没看过该新闻，就推送给用户，如果存在，那么大概率用户看过，就不推送给用户，即使错了，不会造成严重后果，大不了少看一条新闻。
- 垃圾邮件判定，用户每添加一个邮件地址黑名单，就加到布隆过滤器里，来了一封新邮件，问一下过滤器，这个地址是否在黑名单里，是就放到垃圾邮件分类里，如果没有见过，就放到正常邮箱。因此，可能会出现正常邮件被误判的情况，但是垃圾邮件一定被放到了垃圾邮件分类里。

### 1.6.3 基本用法

```
> df.add codehole user1
1
> df.exists codehole user1
1
> df.exists codehole user2
0
```

### 1.6.4 实现原理

布隆过滤器由一个位数组和若干个hash函数组成。每次通过add进来一个元素，使用每个hash函数求得多个hash值，并在hash值对应的数组位置上置为1。当调用exists来查找某个元素是否存在时，又使用每个hash函数求得多个hash值，检查每个hash值对应的数组位置上的值，如果都是1，则判定为存在，如果其中任何一个位置是0，则没有见过。当数组太小，而元素太多时，数组几乎都为1，那么出现误判的几率会大大上升，但是如果判定为不存在，那么这个元素一定不存在，不会出现误判。

### 1.6.5 注意事项

布隆过滤器允许用户设定两个初始化参数：

- initial_size：初始化大小，需要用户去估计里面会装多少个元素，设置得太大，浪费存储空间，需要的hash函数数量也更大，计算效率降低。设置得太小，影响准确率。
- error_rate，错误率，设置得越小，需要的空间越大，但是仍然远远小于set所需空间。

## 1.7 限流器

### 1.7.1 为什么需要限流器

当系统的处理能力有限的时候，需要对请求数量进行限制，因此需要用到限流器。

### 1.7.2 实现方式

两种redis实现的限流器

- 第一种比较简单，使用redis的数据结构zset，将每一次请求的时间作为score，value随意，放到zset里面。当新来一个请求，需要判断它是否能够被处理。假设现在限制1S内只允许10次请求，使用`zremrangebyscore` 来移除时间戳为0-当前时间减1S的范围内的元素，剩下的就是1S之内的请求，如果数量大于10，则拒接此次请求，如果小于则处理，并加入zset。

  这种实现方法的缺点是，如果遇到这种场景，1S之内不能超过100万次，会浪费大量的空间，同时也很耗时。

- 第二种称为漏斗限流法。

  ```python
  class Funnel:
  	capacity:容量
  	leaking_rate：漏水速率
  	left_quota:剩余的水量
    leaking_ts：上次漏水时间
  ```

  我们定义一个漏斗类，拥有上述属性。现在有一个请求进来了，相当于往漏斗里加水

  1. 首先要执行一次漏水操作，也就是根据当前时间，和leaking_ts上次漏水时间的差，乘以漏水速率，算出两次加水时间间隔内，有多少水漏出去了，然后更新left_quota剩余水量，但是剩余水量不能超过总容量。同时，记录leaking_ts为当前时间。
  2. 判断这次加水操作的水量，是否会导致漏斗溢出，如果会，则拒绝加水操作，也就是拒绝请求。如果不会溢出，则执行该请求。
  3. 加水之后，更新剩余水量。

  如果用redis的hash来实现，上面这3步无法保证原子操作，势必要加锁，从而降低性能，而Redis-Cell模块，实现了漏斗算法，并提供了原子的限流指令。

  ```
  > cl.throttle laoqian:reply 15 30 60 1
  laoqian:reply 代表key
  15 代表capacity，漏斗容量
  30 60 代表漏水速率为30/60S
  1 代表此次加水量为1
  
  执行结果返回值会告诉你执行成功还是失败，漏斗总容量，剩余空间，如果被拒绝之后的重试时间，以及需要多久漏斗会完全空出来。
  ```


## 1.8 GeoHash

### 1.8.1 用来干什么

需要用到的场景就是根据地理位置，查找与自己相近的目标，例如附近的人，附近的饭店，离你最近的车。

### 1.8.2 怎么使用

Redis提供了6个Geo指令

```
> geoadd company 116 39 juejin  // 增加坐标点
> geodist company juejin ireader km //求两点的距离
> geopos company juejin // 获取任意坐标点的经纬信息
116
39
> geohash company ireader // 获取元素的经纬度编码字符串
"wx4g52e1ce0"
> georadiusbymember company ireader 20 km count 3 asc // 查询指定元素一定范围内的其他元素
```

### 1.8.3 实现原理

要查找一个元素附近的元素，如果用二维坐标表示元素位置，那么计算量会比用一维坐标系表示元素位置要大。因此，geohash算法的关键就是将二维坐标映射到一维坐标系上。映射的方式为，用二分法不断地将整个平面切成小块，每个小块赋予0或者1，这样一直分下去，编码的长度会越长，但是精度会越高。例如，一个点所在的区域编码是0100，另一个元素的编码是0101，因此这两个点的距离就是1。如果是1000和1100，他们的距离就是二进制0100，就是4。

知道如何编码之后，使用zset数据结构，value是元素，score是它的geohash编码。需要查找时按照zset的查找方式查找即可。

### 1.8.4 注意事项

在一个地图应用中，数据量可能有几百万条，甚至更多，如果使用Redis的Geo数据结构，他们将全部放到一个zset中，如果在Redis的集群环境，集合可能或从一个节点迁移到另一个节点，如果单个key过大，会降低性能，因此在集群环境下，建议单个key的数据量不超过1M，可以按照国家，省份，市来拆分。并且建议不是在单独的Redis实例上，不使用集群环境。

##1.9 scan

### 1.9.1 是什么

提供了一种查找所有符合条件的key的高性能指令。如果不用`scan`指令，redis还提供了`keys`命令来查找，该命令有以下的缺点：

1. 没有offset, limit参数，一次性输出所有符合条件的结果。
2. keys是遍历算法，复杂度是O(n)，如果实例中有千万级的keys，这个指令会导致服务卡顿，所有读写Redis的指令都将被延后甚至超时报错。

### 1.9.2 特点

1. 虽然复杂度是O(n)，但是它通过游标分步进行，不会阻塞线程。
2. 提供limit参数，控制返回结果数量。
3. 提供模式匹配功能。
4. 返回结果可能有重复。
5. 遍历过程中如果有数据修改，不一定能够被遍历到。
6. 单次返回的结果不是空的并不意味着遍历结束，而是要看返回的游标值是否为0。

### 1.9.3 用法

scan提供了三个参数，第一个是cursor游标，第二个是key的正则模式，第三个是遍历的limit hint。第一次遍历cursor是0，将返回结果中第一个整数值作为下次遍历的cursor，一直到返回的cursor的值为0结束。

```
> scan 0 match key99* count 10
1）"13976"
2）1）"key9911"
	 2）"key9974"
	 3）"key9922"
	 4）"key9933"
	 5）"key9944"
> scan 13976 match key99* count 10
1）"1996"
2）1）"key9911"
	 2）"key9974"
	 3）"key9922"
	 4）"key9933"
	 5）"key9944"
```

scan指令是一系列指令，除了可以遍历所有key之外，还可以对指定容器集合进行遍历，比如zscan遍历zset集合，hscan遍历hash字典的元素，sscan会遍历set集合的元素。实现方法类似，因为zset，hash，set内部都是用字典来存储元素。

### 1.9.4 原理

- 存储方式

  Redis中所有的key都存在一个字典中，相当于java中的hashmap，是一位数组加链表的形式存放。scan指令参数中的cursor游标，指的就是就是一位数组中的位置索引下标。limit参数就是指定遍历的数组下标个数。

- 遍历顺序

  scan遍历顺序不是从0开始，而是采用高位进位加法来遍历，之所以这样是考虑到字典的扩容和缩容时，避免重复遍历和遗漏。

- 渐进式rehash

  java的hashmap在扩容时，如果数据量很大，会出现元素卡顿现象。Redis为了解决这个问题，采用了渐进式rehash，也就是同时保留新旧字典，每次移一部分到新字典，查询的时候需要同时查找新旧字典。

# 2. 原理篇

## 2.1 线程IO模型

Redis是**单线程**程序，也就是说一个Redis实例在某一时刻只能用到一个CPU核。但是它确实是拥有高性能的一个中间件，同样是单线程高性能的有Node.js，Nginx。

### 2.1.1 单线程为什么还快

1. Redis的数据，以及操作完全在内存中完成，相比较磁盘读写，速度非常快。
2. 采用单线程，避免了上下文切换和竞争条件，不存在线程切换消耗，以及锁的竞争导致的消耗。
3. 使用多路IO复用模型，非阻塞IO。

### 2.1.2 怎么提升性能

- Redis是单线程的，一个实例只能利用一个CPU，因此应该提升单个CPU的性能，增大机器的内存。
- 启用多个Redis实例，构建Redis集群，让耗时长的操作去某一个slave节点上去执行，这样就不会影响别的任务的执行。

## 2.2 通信协议

Redis采用Redis序列化协议RESP。

###2.2.1 RESP(Redis Serialization Protocol)

Redis协议将传输的结构数据分为5种，单元结束时统一加上回车换行符\r\n

1. 单行字符串以“+”开头。
2. 多行字符串以$符号开头，后跟字符串长度。
3. 整数以“ : ”开头，后面跟整数的字符串形式。
4. 错误消息以"-"符号开头。
5. 数组以“*”号开头，后跟数组的长度。

### 2.2.2 特点

易读性，简单性，易理解性好。虽然有大量的冗余回车换行符，增大了网络传输开销，但是网络流量不是限制数据库性能的瓶颈，因此Redis还是能达到10w/s的超高QPS。

## 2.3 持久化

### 2.3.1 为什么需要持久化？

Redis中的数据都在内存中，如果突然宕机，数据会全部丢失，那么如果想要保证这些数据在宕机时保证数据不丢失，就需要持久化内存中的数据到磁盘。持久化方法分三种，分别是快照，AOF，混合方式。

###2.3.2 快照

快照的实现过程就是，在开始持久化的时刻，制作一个内存快照，然后让子进程去将此时的内存快照写入磁盘，而主进程继续接受处理请求。

那么怎么能够瞬间制作一个内存快照，不会造成主线程卡住呢？

Redis使用了操作系统的COW(Copy on Write)机制来实现内存快照，在Linux上，会调用glibc的函数fork去产生一个子进程，那么对于这个子进程它看到的内存中的数据来说，会一直保持在fork之前，如果主进程在fork之后想要修改内存中的数据，操作系统会将该的内存页复制一份出来，在新的上面修改，保留旧的内存页，因此子进程还是看到的旧数据。如果主进程把所有的内存都更新了，内存中的数据量最多相当于fork之前的2倍。

### 2.3.3 AOF

AOF是一种日志，记录了从某个时刻开始，Redis接收的所有修改数据的指令序列。如果Redis实例宕机了，通过重放AOF日志，来恢复数据。

但是如果Redis实例运行过久，AOF日志量会越来越大。解决方法是，Redis提供了bgrewriteaof指令用于对AOF日志进行重写，实现过程是，开辟一个子进程对内存进行遍历，生成一系列Redis的操作指令，序列化到新的AOF日志中，然后将这段时间内发生的增量AOF日志，追加到新的AOF日志中，追加完毕就立即代替旧的AOF日志。

### 2.3.4 混合持久化

如果使用快照来恢复数据，那么会有大量的数据会丢失，如果使用AOF来恢复数据，会花大量时间来启动Redis。那么混合持久化机制的出现就是为了解决这个问题的，将快照文件和AOF日志放一起，AOF日志是从快照文件开始写的那一刻之后产生的增量AOF，通常这部分很小。

当重启之后，先加载快照文件，再回放AOF文件，重启的效率大大提升。

### 2.3.5 Redis运维

持久化是一个很消耗机器资源的操作，体现在

- fork操作是一个比较耗资源的操作，之后要遍历整个内存，大块写磁盘会加重系统负担。
- fsync是一个比较耗时的IO操作，降低Redis性能的同时，还增加系统IO负担。

因此，持久化通常放到备份子节点去做，子节点因为没有来自客户端的压力，它的机器资源比较充沛。

# 5. 源码篇

## 5.5 zset与跳表

Redis的zset的内部实现是一个hash字典加一个跳表（skiplist）。hash字典用来存储value和score的对应关系。

### 5.5.1 基本结构

![](./pic/5-5跳表.jpg)

- 每一个元素（kv块）可能拥有不同的层高，例如元素3拥有L0，元素6拥有L0，L1。随着层数增加，拥有该层数的kv块数量会越来越少。
- 最底层kv块之间用双向链表串起来（上图中最底下一层的kv块之间的箭头应是双向的），按照从小到大的顺序排列。
- 同一层的kv块会用指针串起来，每一层元素的遍历都是从kv header出发。
- 共有64层，跳表会记录当前的最高层数maxLevel，很少有情况能达到64层，因为一般不会在跳表里放2的64次方个元素。

### 5.5.2 查找过程

- 从当前最高层开始，在每一层寻找那个最后一个比目标元素小的元素，直到在最后一层L0中找到期望的节点。
- 中间经过的一系列节点称为搜索路径，它是每一层最后一个比目标元素小的元素节点列表。有了这个搜索路径就可以确定新插入节点的位置。

### 5.5.3 随机层数

- 对于新插入的节点，需要确定它拥有的层数，Redis采用随机算法给它分配一个合理的层数。
- 首先，一个节点是100%拥有层数L0的，但是能够获得更大一层数的晋升几率只有25%。
- 低的晋升概率，导致了跳表的层数相对较低，在单个层上的遍历次数会稍多一点。

### 5.5.4 插入过程

- 首先要搜索出合适的插入点，将这个过程中的搜索路径找出来。
- 然后创建节点，并为这个节点随机分配一个层数，如果分配的层数高于当前跳表的最大高度，就需要更新当前最大高度。
- 将搜索路径上的节点和这个新节点通过向前向后指针串起来。

### 5.5.5 更新过程

- 调用zadd时，如果对应的value不存在，就是插入过程。
- 如果对应的value存在了，且score发生了变化，则先删除之前的元素，再插入拥有新score的元素。

### 5.5.6 删除过程

- 先找到要删除的节点，并把搜索路径找出来，对于该节点拥有的每一层的相关节点，重排一下前后指针。
- 注意可能会更新maxLevel。

### 5.5.7 元素排名

- Redis给forward指针都赋予了span属性，表示从前一个节点沿着forward指针跳到当前这个节点，中间会跳过多少个节点。Redis在插入，删除操作时都会更新span的值。
- 当要计算一个元素的排名时，只需要将搜索路径经过的所有节点的跨度span值进行叠加，就可以计算出元素的最终rank值。