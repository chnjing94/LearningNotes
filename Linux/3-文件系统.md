#3. 文件系统

##3.1 概述

内存中的数据在进程结束时，或者机器掉电时会丢失，如果想要永久保存这些数据，就需要将数据持久化，也就是写入文件。操作系统的文件系统就是这样一个存放数据的档案库，负责管理外部存储设备上的数据。常见的外部存储设备有硬盘，u盘等。

### 3.1.1 文件系统功能划分

为了管理文件，文件系统应该考虑到以下几点：

1. 文件系统要有严格的组织形式，使得文件能够以块为单位进行存储。

2. 文件系统要有索引区，用来方便查找一个文件分成的多个块的存储位置。如下图所示，

   ![](./pic/3-文件系统索引.png)

3. 文件系统应该有缓存层，用来存储那些近期经常被读取和写入的热点文件。

4. 文件系统应该用文件夹的形式组织起来，方便管理和查询。使用文件夹还可以通过将同名文件放在不同文件夹下，来避免重名冲突。

   ![](./pic/3-文件夹目录.png)

5. 内核要在自己内存里维护一套数据结构，来保存哪些文件被哪些进程打开和使用。

### 3.1.2 文件系统相关命令行

```c
fdisk -l 						 // 查看硬盘分区情况
mkfs.ext3或mkfs.ext4 // 对分区进行格式化
mount 							 // 将硬盘挂载在某个目录下
unmount							 // 卸载某个硬盘
ls -l 							 // 查看当前目录下所有文件的信息
```

在Linux里，一切都是文件，通过ls -l结果的第一位可以看出

- \- 表示普通文件
- d 表示文件夹
- c 表示字符设备文件
- b 表示块设备文件
- s 表示套接字socket文件
- l 表示符号链接

### 3.1.3 相关系统调用

- open，打开一个文件，并返回一个文件描述符fd。

  ```c
  fd=open("./test", O_RDWR|O_CREATE|O_TRUNC)
  O_CREATE代表不存在时，创建一个新文件。
  O_RDWR表示以读写方式打开
  O_TRUNC表示打开文件后，将文件长度截断为0
  ```

- write，写入数据，第一个参数是文件描述符，第二个参数是要写入的数据的存放位置，第三个参数表示希望写入的字节数。返回值是成功写入文件的字节数。

- lseek，重新定位读写的位置。

- read，读取数据。第一个参数是文件描述符，第二个参数是读取来的数据存放的空间，第三个参数是希望读取的字节数。返回值是成功读取的字节数。

- close，关闭一个文件。

- stat，通过文件名查询状态信息。lstat可以处理软连接。fstat，通过文件描述符查询文件信息。

- opendir 打开一个目录, 生成一个目录流 DIR

- readdir 读取目录流的一个条目, 自动指向下一个条目

- closedir 关闭目录流



文件系统总体如下

![](./pic/3-文件系统概述.png)

## 3.2 硬盘文件系统

这一节主要讲Linux下最主流的文件系统格式——ext系列文件系统格式。以下以传统机械硬盘为例。

### 3.2.1 inode与块的存储

硬盘分成相同大小的单元，称为块(block)，一个块的大小是扇区大小的整数倍，默认是4k。这个值可以在格式化的时候设定。文件可以存放在不连续的块中，不一定要分配连续的空间，这样比较灵活。

但是文件存放在哪些block中，需要记录下来，还需要建立索引快速查找。这个时候就需要一个结构体inode来存放。inode里还存放有文件的元数据如名字，权限等。inode中的i是index的意思，inode定义如下

```c
struct ext4_inode {
	__le16	i_mode;		/* File mode */
	__le16	i_uid;		/* Low 16 bits of Owner Uid */
	__le32	i_size_lo;	/* Size in bytes */
	__le32	i_atime;	/* Access time */
	__le32	i_ctime;	/* Inode Change time */
	__le32	i_mtime;	/* Modification time */
	__le32	i_dtime;	/* Deletion Time */
	__le16	i_gid;		/* Low 16 bits of Group Id */
	__le16	i_links_count;	/* Links count */
	__le32	i_blocks_lo;	/* Blocks count */
	__le32	i_flags;	/* File flags */
......
	__le32	i_block[EXT4_N_BLOCKS];/* Pointers to blocks */
	__le32	i_generation;	/* File version (for NFS) */
	__le32	i_file_acl_lo;	/* File ACL */
	__le32	i_size_high;
......
};
```

里面指定了文件的读写权限，属于哪个用户，哪个组，大小，占用多少块，跟时间相关的信息。当我们执行ls命令的时候，就是从这里取出信息的。

i_block这个变量里，存放了文件被分成了哪些块，每一块在哪里的信息。

![](./pic/3-i_block.jpeg)

如上图所示，在ext3中，前12项直接存放块地址，如果块太多就会在12-14项存放间接块，像树的结果那样存放大量的块信息。这样的缺点就是当块太多是，需要多次读盘，速度太慢。

为了解决这个问题，ext4引入了新的概念Extents，可以把最大128M的数据放到一个Extents里面，对大文件的读写性能提高了，文件碎片页减少了。

### 3.2.2 inode位图和块位图

当需要新建文件的时候，需要找到一个未使用的inode，然后分配未使用的块。为了找到未使用的inode和块，在文件系统里，专门用一个块，也就是4k来存储inode位图，0表示未使用，1代表已使用。同样的，也有一个块用来保存块的位图。需要inode就去位图里找，需要块就去块位图里找。

### 3.2.3 文件系统格式

我们把一个块的位图+一系列的块，外加一个块的inode位图+一系列的inode的结构称为一个块组，可以表示128M的数据，有N多的块组，可以表示N大的文件，如下图：

![](./pic/3-块组.jpeg)

一个个块组构成了文件系统。块组描述符保存了当前块组的inode位图，块位图等信息。

块组描述符表是所有的块组描述符组成的列表，超级块描述了整个文件系统，如一共有多少个块，每个块组有多少个inode等信息，这些信息都是全局信息。

因为这些信息很重要，如果这些数据丢失，整个文件系统都打不开了。于是这些数据都要备份到其他多个块组中，这样一来就产生了两个问题：

- 浪费空间
- 块组描述符表的大小，限制了整个文件系统的大小。

于是，引入了Meta Block Groups特性。

块组描述符表不会保存所有块组的描述符了，而是将块组分为多个组，称为元块组。如图：

![](./pic/3-元块组.jpeg)

每个元块组包含64个块组，块组描述符也是64项，备份三份，在元块组的第1，2和最后一块的开始处。

ext4采用48位块寻址，高达1EB的空间。

### 3.2.4 目录的存储格式

目录本身也是一个文件，也有inode，只是在数据块里存储的数据，和普通文件不一样，为了加快目录下文件的查找速度，将文件按hash值分到不同的块中。如下图所示：

![](./pic/3-目录存储方式.jpeg)

- 首先出现的是两个文件名为“.”和“..”的目录文件，分别指向当前目录和上级目录。
- 当我们要查找文件夹下某个文件时，先计算其hash值，再到对应的块里面去查，这样可以加快查找速度。

###3.2.5 软链接和硬链接的存储格式

使用`ln [参数][源文件或目录][目标文件或目录]` 来创建链接。ln -s是创建软链接，不加-s是硬链接。他们的区别如下：

- 硬链接和原始文件共用一个inode，但是inode是不跨文件系统的，因此硬链接没法跨文件系统。
- 软链接相当于重新创建了一个文件，这个的内容是指向另一个文件的路径。可以跨文件系统，甚至目标文件被删除了，链接文件还是在的。

![](./pic/3-软硬链接.jpeg)

##3.3 虚拟文件系统

上一节讲的是ext格式的文件系统，实际上，Linux支持十种文件系统如ext3，ext4，ntfs等。它们的实现各不相同，因此Linux内核向用户空间提供了虚拟文件系统这个统一的接口，来对文件系统进行操作。它提供了场景的文件系统对象模型，如inode，directory entry，mount等，以及操作这些对象的方法。

在文件读写的流程中，需要多个组件一起合作，虚拟文件系统VFS的位置如下：
![](./pic/3-虚拟文件系统.jpg)

- 在用户态，通过系统调用如sys_open，sys_read来对文件进行操作。
- 在内核，每个进程都要为打开的文件，维护一定的数据结构(task_struct->*files)。
- 在内核，还要维护系统打开的文件的数据结构struct file。
- 虚拟文件系统提供统一的文件操作接口。
- 然后是真正对接的文件系统，如ext4，ext3，ntfs。
- 为了加快块设备的读写效率，还有一个缓存层。
- 为了读写文件系统，要通过块设备IO层。这是文件系统层和块设备驱动层的接口。
- 最下层是块设备驱动程序

通过mount和open系统调用来了解整个系统的架构。

### 3.3.1 挂载文件系统

挂载文件操作，在虚拟机与宿主机共享文件夹时有用到过，挂载文件目录到docker容器里用到过，还在新建磁盘分区时用到过。想要操作文件系统，第一件事就是挂载文件系统。

内核是不是支持某种文件系统，需要向内核进行注册才知道。

挂载文件系统是调用mount系统调用，其调用链为do_mount->do_new_mount->vfs_kern_mount。

vfs_kern_mount定义如下

```c
struct vfsmount *
vfs_kern_mount(struct file_system_type *type, int flags, const char *name, void *data)
{
......
	mnt = alloc_vfsmnt(name);
......
	root = mount_fs(type, flags, name, data);
......
	mnt->mnt.mnt_root = root;
	mnt->mnt.mnt_sb = root->d_sb;
	mnt->mnt_mountpoint = mnt->mnt.mnt_root;
	mnt->mnt_parent = mnt;
	list_add_tail(&mnt->mnt_instance, &root->d_sb->s_mounts);
	return &mnt->mnt;
}

```

先创建了一个struct mount 结构，每个挂载的文件系统都对应这样一个结构。

```c
struct mount {
	struct hlist_node mnt_hash;
	struct mount *mnt_parent;
	struct dentry *mnt_mountpoint;
	struct vfsmount mnt;
	union {
		struct rcu_head mnt_rcu;
		struct llist_node mnt_llist;
	};
	struct list_head mnt_mounts;	/* list of children, anchored here */
	struct list_head mnt_child;	/* and going through their mnt_child */
	struct list_head mnt_instance;	/* mount instance on sb->s_mounts */
	const char *mnt_devname;	/* Name of device e.g. /dev/dsk/hda1 */
	struct list_head mnt_list;
......
} __randomize_layout;


struct vfsmount {
	struct dentry *mnt_root;	/* root of the mounted tree */
	struct super_block *mnt_sb;	/* pointer to superblock */
	int mnt_flags;
} __randomize_layout;
```

- mnt_parent是挂载点所在的父文件系统。
- mnt_mountpoint是挂载点在父文件系统中的dentry，dentry表示目录。
- mnt_root是当前文件系统根目录的dentry。
- mnt_sb是指向超级块的指针。

调用mount_fs挂载文件系统

```c
struct dentry *
mount_fs(struct file_system_type *type, int flags, const char *name, void *data)
{
	struct dentry *root;
	struct super_block *sb;
......
	root = type->mount(type, flags, name, data);
......
	sb = root->d_sb;
......
}
```

调用type->mount对于ext4来说就是调用ext4_mount。从文件系统里面读取超级块。当所有的数据结构都读到内存里面，内核就可以通过操作这些数据结构，来操作文件系统了。

假设，在根目录下挂载了根文件系统，在/home下挂载了文件系统A，在/home/hello下挂载了文件系统B，还有/home/hello/world和/home/hello/world/data两个文件夹。它们之间就有如下的关系:

![](./pic/3-mount关系图.jpeg)

### 3.3.2 打开文件

打开文件对应的是open系统调用，最终对应到内核的系统调用实现sys_open，过程如下：

首先调用get_unused_fd_flags，找到一个未使用的文件描述符fd。如何找？

在每个进程的task_struct中有一个指针*files，指向进程打开的文件描述符列表(数组)

```c
struct files_struct		*files;

struct files_struct {
......
	struct file __rcu * fd_array[NR_OPEN_DEFAULT];
};

```

每打开一个文件，就会在这个列表中分配一项，这一项的下标就是文件描述符，所以fd其实就是一个整数。对于任何一个进程，默认情况下，文件描述符0表示标准输入stdin，1表示标准输出stdout，2表示标准错误输出stderr。

拿到分配的fd后，需要创建一个struct file结构，然后将fd和这个file关联起来。

![](./pic/3-struct file.png)

创建过程如下：

1. 调用get_empty_filp生成一个struct file结构
2. 初始化nameidata里面的path变量，准备开始节点路径查找。
3. 调用link_path_walk对路径名逐层进行节点路径查找，一直找到最后一级的父目录，例如'/root/hello/world/data'，找到/root/hello/world为止。
4. 最后一部分调用do_last获取文件对应的inode对象，并初始化file对象。

do_last先查找文件路径最后一部分对应的dentry，如何查找?

- 先去cache里面查找，也就是lookup_fast。
- 如果没找到，就要去文件系统里查找。先新建一个dentry，并且调用上一级目录inode的inode_operations的lookup函数去找该文件的inode，对于ext4来说，调用的是ext4_lookup，得到inode指针放到新dentry里。最后将新的dentry赋给file->path->dentry变量。

找到dentry的目的就是，里面有指向该文件inode的指针，文件关闭之后entry会放到cache中，因而可以长期维持内存中的文件和硬盘上的文件的关系。最后调用vfs_open真正打开文件。里面最重要的事是调用f_op->open，然后将打开文件的所有信息填写到struct file这个结构里。

虚拟文件系统整体数据结构如下:

![](./pic/3-虚拟文件系统数据结构.png)

## 3.4 文件缓存

文件的读写与文件缓存密切相关，这里通过分析文件读写过程来了解文件缓存。

文件的读写的调用流程如下：

- 在系统调用层调用read/write。

- 在虚拟文件系统层调用`vfs_read->__vfs_read`和`vfs_write->__vfs_write`，对于每个打开的文件都有一个struct file结构与之对应，里面又一个struct file_operations f_op，用于定义对这个文件做的操作。`__vfs_read`会调用file_operations里的read操作，`__vfs_write`会调用file_operations里的write操作。

- 到了具体文件系统层，如exr4文件系统，内核定义了一个ext4_file_operations。

  ```c
  const struct file_operations ext4_file_operations = {
  ......
  	.read_iter	= ext4_file_read_iter,
  	.write_iter	= ext4_file_write_iter,
  ......
  }
  ```

  读就调用ext4_file_read_iter，然后调用generic_file_read_iter。

  写就调用ext4_file_write_iter，然后调用__generic_file_write_iter。

  generic_file_read_iter和__generic_file_write_iter有相似逻辑，就是要区分是否用缓存。

### 3.4.1 缓存

缓存就是内存中的一块区域，因为读写内存的速度比硬盘要快很多，为了提高读写性能，Linux将部分读写的操作放在缓存中，不是每一次都直接访问硬盘。

根据是否使用缓存，我们可以把文件I/O操作分为两种类型：

1. 缓存I/O，读数据时先检查缓冲区有没有数据，如果有就直接返回，没有就从硬盘中读取，然后存到缓存中。写操作，会将数据从用户空间复制到内核空间的缓存中，就算完成写操作了，操作系统会决定什么时候真正写到磁盘，用户也可以显式调用sync命令，强制立刻刷盘。
2. 直接I/O，就是应用程序直接访问磁盘数据，省去了数据在用户空间和内核空间的转换。

对于缓存来说，需要文件和内存页进行关联，需要用到address_space。在ext4中，对address_space的操作定义在ext4_aops，direct_IO对应的函数是ext4_direct_IO。

在写逻辑里，如果发现设置了IOCN_DIRECT，就调用direct_IO，将数据直接写入硬盘。ext4_direct_IO会跨过缓存层，直接调用块设备的驱动程序。

### 3.4.2 带缓存的写入操作

带缓存的写入操作generic_perform_write。

```c
ssize_t generic_perform_write(struct file *file,
				struct iov_iter *i, loff_t pos)
{
	struct address_space *mapping = file->f_mapping;
	const struct address_space_operations *a_ops = mapping->a_ops;
	do {
		struct page *page;
		unsigned long offset;	/* Offset into pagecache page */
		unsigned long bytes;	/* Bytes to write to page */
		status = a_ops->write_begin(file, mapping, pos, bytes, flags,
						&page, &fsdata);
		copied = iov_iter_copy_from_user_atomic(page, i, offset, bytes);
		flush_dcache_page(page);
		status = a_ops->write_end(file, mapping, pos, bytes, copied,
						page, fsdata);
		pos += copied;
		written += copied;


		balance_dirty_pages_ratelimited(mapping);
	} while (iov_iter_count(i));
}
```

这个函数里是一个大循环，需要找出这次写入影响到的所有的页，然后依次写入。每次循环做以下几件事：

- 对于每一页，调用address_space的write_begin做一些准备。

  在内核中，缓存以页为单位放在内存里，如何知道哪些数据放到缓存中了？每一个打开的文件都有一个struct file结构，里面有一个struct address_space用于关联文件和内存。在这个结构里，还有一棵树，用于保存所有与这个文件相关的缓存页。查找页的时候，就是根据文件偏移量找出相对应的页，如果找不到就创建一个缓存页。

- 调用iov_iter_copy_from_user_atomic，将写入的内容从用户态拷贝到内核态。

  调用kmap_atomic将分配的页映射到内核里的一个虚拟地址，然后将用户态数据拷贝到内核态页面的虚拟地址中，用完后调用kunmap_atomic把内核里面的映射删除。

- 调用address_space的write_end完成写操作。将修改过的缓存标记为脏页，此时并没有真正写入硬盘。

- 调用balance_dirty_pages_ratelimited，看脏页是否太多，需要写回硬盘。如果太多，就启动一个后台线程开始回写(write back)。一下几种场景也会触发回写：

  1. 用户主动调用sync，将脏页刷盘。
  2. 内存十分紧张以至于无法分配页面，需要将脏页刷盘，然后释放脏页。
  3. 脏页更新了较长时间，超过了timer，需要及时写回。

### 3.4.3 带缓存的读操作

带缓存的读操作，对应的函数时generic_file_buffered_read，

```c
static ssize_t generic_file_buffered_read(struct kiocb *iocb,
		struct iov_iter *iter, ssize_t written)
{
	struct file *filp = iocb->ki_filp;
	struct address_space *mapping = filp->f_mapping;
	struct inode *inode = mapping->host;
	for (;;) {
		struct page *page;
		pgoff_t end_index;
		loff_t isize;
		page = find_get_page(mapping, index);
		if (!page) {
			if (iocb->ki_flags & IOCB_NOWAIT)
				goto would_block;
			page_cache_sync_readahead(mapping,
					ra, filp,
					index, last_index - index);
			page = find_get_page(mapping, index);
			if (unlikely(page == NULL))
				goto no_cached_page;
		}
		if (PageReadahead(page)) {
			page_cache_async_readahead(mapping,
					ra, filp, page,
					index, last_index - index);
		}
		/*
		 * Ok, we have the page, and it's up-to-date, so
		 * now we can copy it to user space...
		 */
		ret = copy_page_to_iter(page, offset, nr, iter);
    }
}
```

在generic_file_buffered_read里，先去找是否有缓存页。如果没有，不但读取这一页还要进行预读。预读完后，再尝试一把查找缓存页。如果第一次找缓存页就找到了，还是要判断是否需要预读。

最后，copy_page_to_iter会将内容从内核缓存页拷贝到用户内存空间。

